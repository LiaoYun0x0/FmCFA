Multimodal image feature matching is an important technique in computer vision. However, many methods nowadays implement intensive attention interaction, but information interactions in non-critical areas attract noise and wastes computing resources. In contrast, focusing the information interaction on key regions (information-rich regions) will bring great benefits to the subsequent matching phase. Based on this, we design a feature matching method for critical feature attention interactions called FmCFA for multimodal images. We propose a new critical feature attention (CF-Attention) to focus attention interactions on critical regions. Further, CFa-Block is proposed based on CF-Attention to accomplish coarse-grained information interaction. These enable FmCFA to complete the matching task accurately and efficiently. Finally, a large number of experiments show that FmCFA performs particularly well under multiple modal image datasets.
